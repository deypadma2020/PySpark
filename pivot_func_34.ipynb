{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ed608e",
   "metadata": {},
   "source": [
    "- The pivot() function in PySpark is used to rotate (pivot) a DataFrame by turing unique values from one column into separate columns. It's commonly used for data summarization and reporting.\n",
    "\n",
    "- when you pivot data, you group it by some columns and create new columns for each unique value in another column. Typically, an aggregate function (like sum, avg) is applied to fill in the values.\n",
    "\n",
    "- syntax:\n",
    "    - df.groupBy(<group_columns>).pivot(<pivot_column>).agg(<aggregate_functions>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6022e1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/09 15:08:54 WARN Utils: Your hostname, KLZPC0015, resolves to a loopback address: 127.0.1.1; using 172.25.17.96 instead (on interface eth0)\n",
      "25/09/09 15:08:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/09 15:09:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/09/09 15:09:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/09/09 15:09:04 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/09/09 15:09:04 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/09/09 15:09:04 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PivotFunctionExample\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d9eb9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:>                                                         (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----+---------+-------+\n",
      "| id|     name|year|  product|revenue|\n",
      "+---+---------+----+---------+-------+\n",
      "|  1|    Manta|2023|Product A|    240|\n",
      "|  2| Dipankar|2024|Product A|    270|\n",
      "|  3|   Souvik|2022|Product B|    270|\n",
      "|  4|Soukarjya|2025|Product A|   NULL|\n",
      "|  5|   Arvind|2022|Product C|    280|\n",
      "|  6| Prodipta|2022|Product B|    280|\n",
      "|  7|    Padma|2025|Product A|    270|\n",
      "|  8|    Panta|2023|Product C|    270|\n",
      "|  9|  Sougato|2022|Product B|    290|\n",
      "+---+---------+----+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# create sample dataframes\n",
    "data = [\n",
    "    (1, \"Manta\", 2023, \"Product A\", 240),\n",
    "    (2, \"Dipankar\", 2024, \"Product A\", 270),\n",
    "    (3, \"Souvik\", 2022, \"Product B\", 270),\n",
    "    (4, \"Soukarjya\", 2025, \"Product A\", None),\n",
    "    (5, \"Arvind\", 2022, \"Product C\", 280),\n",
    "    (6, \"Prodipta\", 2022, \"Product B\", 280),\n",
    "    (7, \"Padma\", 2025, \"Product A\", 270),\n",
    "    (8, \"Panta\", 2023, \"Product C\", 270),\n",
    "    (9, \"Sougato\", 2022, \"Product B\", 290)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"year\", \"product\", \"revenue\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdec812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+---------+---------+\n",
      "|     name|year|Product A|Product B|Product C|\n",
      "+---------+----+---------+---------+---------+\n",
      "|   Arvind|2022|     NULL|     NULL|      280|\n",
      "|    Manta|2023|      240|     NULL|     NULL|\n",
      "|Soukarjya|2025|     NULL|     NULL|     NULL|\n",
      "|   Souvik|2022|     NULL|      270|     NULL|\n",
      "|  Sougato|2022|     NULL|      290|     NULL|\n",
      "| Dipankar|2024|      270|     NULL|     NULL|\n",
      "|    Padma|2025|      270|     NULL|     NULL|\n",
      "|    Panta|2023|     NULL|     NULL|      270|\n",
      "| Prodipta|2022|     NULL|      280|     NULL|\n",
      "+---------+----+---------+---------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# we pivot the \"product\" column to turnunique product names into columns and sum the \"revenue\" for each combination of name and year\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "pivot_df = df.groupBy(\"name\", \"year\") \\\n",
    "             .pivot(\"product\") \\\n",
    "             .agg(sum(\"revenue\"))\n",
    "\n",
    "pivot_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce45fa77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----+----+----+\n",
      "|     name|2022|2023|2024|2025|\n",
      "+---------+----+----+----+----+\n",
      "|    Padma|NULL|NULL|NULL| 270|\n",
      "|    Manta|NULL| 240|NULL|NULL|\n",
      "|    Panta|NULL| 270|NULL|NULL|\n",
      "|  Sougato| 290|NULL|NULL|NULL|\n",
      "|Soukarjya|NULL|NULL|NULL|NULL|\n",
      "| Prodipta| 280|NULL|NULL|NULL|\n",
      "|   Souvik| 270|NULL|NULL|NULL|\n",
      "| Dipankar|NULL|NULL| 270|NULL|\n",
      "|   Arvind| 280|NULL|NULL|NULL|\n",
      "+---------+----+----+----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# we pivot the \"year\" column to turn years into columns and sum the \"revenue\" for each name across different years\n",
    "\n",
    "pivot_df_year = df.groupBy(\"name\") \\\n",
    "                  .pivot(\"year\") \\\n",
    "                  .agg(sum(\"revenue\"))\n",
    "\n",
    "pivot_df_year.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6365fa7d",
   "metadata": {},
   "source": [
    "- pivot() - reshapes DatafRame by turning unique values from a column into separate columns.\n",
    "- can perform aggregations like sum(), avg(), min(), etc.\n",
    "- Useful for reporting, summarizing data and preparing data for BI tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b878fff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
