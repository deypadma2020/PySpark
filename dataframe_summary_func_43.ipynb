{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9051dd41",
   "metadata": {},
   "source": [
    "- What is summary() function in PySpark?\n",
    "    - The summary(*statistics) function in PySpark provides descriptive statistics for numeric and columns in a DataFrame.\n",
    "\n",
    "- By default, it shows the following statistics:\n",
    "    - count: total number of rows\n",
    "    - mean: average of numeric columns\n",
    "    - stddev: standard deviation of numeric columns\n",
    "    - min: minimum value\n",
    "    - max: maximum value\n",
    "\n",
    "- can also specify own statistics, like 'count', '25%'(first quartile), '50%'(median), '75%'(third quartile), etc.\n",
    "\n",
    "- Syntax: \n",
    "    DataFrame.summary(*statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "335d4547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/16 09:03:50 WARN Utils: Your hostname, KLZPC0015, resolves to a loopback address: 127.0.1.1; using 172.25.17.96 instead (on interface eth0)\n",
      "25/09/16 09:03:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/16 09:04:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataFrameSummaryExample\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7617f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+\n",
      "|age|     name|salary|\n",
      "+---+---------+------+\n",
      "| 29| Dipankar| 35000|\n",
      "| 28| Prodipta| 25000|\n",
      "| 27|    Padma| 20000|\n",
      "| 26|   Souvik| 70000|\n",
      "| 28|Soukarjya| 65000|\n",
      "+---+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (29, \"Dipankar\", 35000),\n",
    "    (28, \"Prodipta\", 25000),\n",
    "    (27, \"Padma\", 20000),\n",
    "    (26, \"Souvik\", 70000),\n",
    "    (28, \"Soukarjya\", 65000)\n",
    "]\n",
    "\n",
    "columns = [\"age\", \"name\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "512daf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default summary statistics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/16 09:05:07 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------+------------------+\n",
      "|summary|              age|    name|            salary|\n",
      "+-------+-----------------+--------+------------------+\n",
      "|  count|                5|       5|                 5|\n",
      "|   mean|             27.6|    NULL|           43000.0|\n",
      "| stddev|1.140175425099138|    NULL|23075.961518428652|\n",
      "|    min|               26|Dipankar|             20000|\n",
      "|    25%|               27|    NULL|             25000|\n",
      "|    50%|               28|    NULL|             35000|\n",
      "|    75%|               28|    NULL|             65000|\n",
      "|    max|               29|  Souvik|             70000|\n",
      "+-------+-----------------+--------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Using summary() function in Pyspark\n",
    "# Example: default summary statistics(count, mean, stddev, min, max)\n",
    "print(\"Default summary statistics:\")\n",
    "df.summary().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c0b714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom summary statistics(count, min, max): \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------+------+\n",
      "|summary|age|    name|salary|\n",
      "+-------+---+--------+------+\n",
      "|  count|  5|       5|     5|\n",
      "|    min| 26|Dipankar| 20000|\n",
      "|    max| 29|  Souvik| 70000|\n",
      "+-------+---+--------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Example: Custom Statistics - only 'count', 'min', 'max'\n",
    "print(\"Custom summary statistics(count, min, max): \")\n",
    "df.summary(\"count\", \"min\", \"max\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3297ad97",
   "metadata": {},
   "source": [
    "- Summary() is used to get descriptive statistics of your data.\n",
    "- It works on numeric columns and also returns counts on string columns.\n",
    "- You can specify which statistics you want.\n",
    "\n",
    "- Example Recap:\n",
    "    - We created a DataFrame with age, name, and salary.\n",
    "    - We used df.summary() to get quick insights into the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e71ee2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
